"""
Evaluation Agent for RAGAS and DEEPEVAL metrics assessment.
This agent evaluates responses generated by other agents in the swarm.
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool

from core.config import config
from core.evaluation_tools import (
    RAGASEvaluationTools,
    DEEPEVALEvaluationTools,
    CustomEvaluationTools,
    EvaluationResult
)


class EvaluationAgent:
    """
    Agent specialized in evaluating response quality using RAGAS and DEEPEVAL.
    Can be called by other agents to assess their responses.
    """
    
    def __init__(self):
        self.logger = logging.getLogger("evaluation_agent")
        self.llm = ChatAnthropic(
            model=config.anthropic.model,
            api_key=config.anthropic.api_key,
            temperature=config.anthropic.temperature,
            max_tokens=config.anthropic.max_tokens
        )
        
        # Initialize evaluation tools
        self.ragas_tools = RAGASEvaluationTools()
        self.deepeval_tools = DEEPEVALEvaluationTools()
        self.custom_tools = CustomEvaluationTools()
        
        # Define available tools
        self.tools = [
            self.evaluate_rag_quality,
            self.evaluate_llm_quality,
            self.evaluate_comprehensive,
            self.get_evaluation_summary,
            self.suggest_improvements
        ]
    
    @tool
    async def evaluate_rag_quality(
        self, 
        query: str, 
        response: str, 
        context: List[str],
        agent_name: str
    ) -> Dict[str, Any]:
        """
        Evaluate RAG quality using RAGAS metrics.
        
        Args:
            query: The original query
            response: The response to evaluate
            context: Context documents used for the response
            agent_name: Name of the agent that generated the response
        
        Returns:
            Dictionary with RAG evaluation results
        """
        try:
            self.logger.info(f"Evaluating RAG quality for agent: {agent_name}")
            
            # Run comprehensive RAG evaluation
            results = await self.ragas_tools.evaluate_comprehensive_rag(
                query, response, context, agent_name
            )
            
            # Convert to dictionary format
            evaluation_data = {
                "agent_name": agent_name,
                "query": query,
                "response": response,
                "context_count": len(context),
                "evaluation_timestamp": datetime.now().isoformat(),
                "metrics": {}
            }
            
            for result in results:
                evaluation_data["metrics"][result.metric_name] = {
                    "score": result.score,
                    "details": result.details
                }
            
            # Calculate overall RAG score
            if results:
                scores = [r.score for r in results if r.score > 0]
                overall_score = sum(scores) / len(scores) if scores else 0.0
                evaluation_data["overall_rag_score"] = overall_score
            
            return evaluation_data
            
        except Exception as e:
            self.logger.error(f"Error in RAG quality evaluation: {str(e)}")
            return {
                "error": str(e),
                "agent_name": agent_name,
                "query": query,
                "response": response
            }
    
    @tool
    async def evaluate_llm_quality(
        self, 
        query: str, 
        response: str,
        agent_name: str
    ) -> Dict[str, Any]:
        """
        Evaluate LLM quality using DEEPEVAL metrics.
        
        Args:
            query: The original query
            response: The response to evaluate
            agent_name: Name of the agent that generated the response
        
        Returns:
            Dictionary with LLM evaluation results
        """
        try:
            self.logger.info(f"Evaluating LLM quality for agent: {agent_name}")
            
            # Run comprehensive LLM evaluation
            results = await self.deepeval_tools.evaluate_comprehensive_llm(
                query, response, agent_name
            )
            
            # Convert to dictionary format
            evaluation_data = {
                "agent_name": agent_name,
                "query": query,
                "response": response,
                "evaluation_timestamp": datetime.now().isoformat(),
                "metrics": {}
            }
            
            for result in results:
                evaluation_data["metrics"][result.metric_name] = {
                    "score": result.score,
                    "details": result.details
                }
            
            # Calculate overall LLM score
            if results:
                scores = [r.score for r in results if r.score > 0]
                overall_score = sum(scores) / len(scores) if scores else 0.0
                evaluation_data["overall_llm_score"] = overall_score
            
            return evaluation_data
            
        except Exception as e:
            self.logger.error(f"Error in LLM quality evaluation: {str(e)}")
            return {
                "error": str(e),
                "agent_name": agent_name,
                "query": query,
                "response": response
            }
    
    @tool
    async def evaluate_comprehensive(
        self, 
        query: str, 
        response: str, 
        context: Optional[List[str]] = None,
        agent_name: str = "unknown"
    ) -> Dict[str, Any]:
        """
        Run comprehensive evaluation using all available metrics.
        
        Args:
            query: The original query
            response: The response to evaluate
            context: Context documents (optional)
            agent_name: Name of the agent that generated the response
        
        Returns:
            Dictionary with comprehensive evaluation results
        """
        try:
            self.logger.info(f"Running comprehensive evaluation for agent: {agent_name}")
            
            evaluation_data = {
                "agent_name": agent_name,
                "query": query,
                "response": response,
                "context_count": len(context) if context else 0,
                "evaluation_timestamp": datetime.now().isoformat(),
                "evaluation_type": "comprehensive",
                "metrics": {}
            }
            
            # RAG evaluation (if context provided)
            if context:
                rag_results = await self.ragas_tools.evaluate_comprehensive_rag(
                    query, response, context, agent_name
                )
                for result in rag_results:
                    evaluation_data["metrics"][f"rag_{result.metric_name}"] = {
                        "score": result.score,
                        "details": result.details
                    }
            
            # LLM evaluation
            llm_results = await self.deepeval_tools.evaluate_comprehensive_llm(
                query, response, agent_name
            )
            for result in llm_results:
                evaluation_data["metrics"][f"llm_{result.metric_name}"] = {
                    "score": result.score,
                    "details": result.details
                }
            
            # Custom evaluation
            custom_results = [
                await self.custom_tools.evaluate_response_completeness(query, response, agent_name),
                await self.custom_tools.evaluate_response_coherence(query, response, agent_name)
            ]
            for result in custom_results:
                evaluation_data["metrics"][f"custom_{result.metric_name}"] = {
                    "score": result.score,
                    "details": result.details
                }
            
            # Calculate overall scores
            all_scores = []
            for metric_name, metric_data in evaluation_data["metrics"].items():
                if isinstance(metric_data, dict) and "score" in metric_data:
                    score = metric_data["score"]
                    if score > 0:  # Only include valid scores
                        all_scores.append(score)
            
            if all_scores:
                evaluation_data["overall_score"] = sum(all_scores) / len(all_scores)
                evaluation_data["score_count"] = len(all_scores)
            
            return evaluation_data
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive evaluation: {str(e)}")
            return {
                "error": str(e),
                "agent_name": agent_name,
                "query": query,
                "response": response
            }
    
    @tool
    async def get_evaluation_summary(
        self, 
        evaluation_results: Dict[str, Any]
    ) -> str:
        """
        Generate a human-readable summary of evaluation results.
        
        Args:
            evaluation_results: Results from evaluation methods
        
        Returns:
            Human-readable summary string
        """
        try:
            if "error" in evaluation_results:
                return f"Evaluation failed: {evaluation_results['error']}"
            
            agent_name = evaluation_results.get("agent_name", "Unknown")
            overall_score = evaluation_results.get("overall_score", 0.0)
            metrics = evaluation_results.get("metrics", {})
            
            summary_parts = [
                f"Evaluation Summary for {agent_name}:",
                f"Overall Score: {overall_score:.2f}/1.0",
                "",
                "Detailed Metrics:"
            ]
            
            for metric_name, metric_data in metrics.items():
                if isinstance(metric_data, dict) and "score" in metric_data:
                    score = metric_data["score"]
                    summary_parts.append(f"  - {metric_name}: {score:.2f}/1.0")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            self.logger.error(f"Error generating evaluation summary: {str(e)}")
            return f"Error generating summary: {str(e)}"
    
    @tool
    async def suggest_improvements(
        self, 
        evaluation_results: Dict[str, Any]
    ) -> str:
        """
        Suggest improvements based on evaluation results.
        
        Args:
            evaluation_results: Results from evaluation methods
        
        Returns:
            Suggestions for improvement
        """
        try:
            if "error" in evaluation_results:
                return "Cannot suggest improvements due to evaluation error."
            
            metrics = evaluation_results.get("metrics", {})
            suggestions = []
            
            # Check specific metrics and provide suggestions
            for metric_name, metric_data in metrics.items():
                if isinstance(metric_data, dict) and "score" in metric_data:
                    score = metric_data["score"]
                    
                    if score < 0.5:  # Low score
                        if "faithfulness" in metric_name:
                            suggestions.append("- Improve response accuracy by using more reliable sources")
                        elif "relevancy" in metric_name:
                            suggestions.append("- Make responses more directly relevant to the query")
                        elif "hallucination" in metric_name:
                            suggestions.append("- Reduce hallucination by sticking to provided context")
                        elif "bias" in metric_name:
                            suggestions.append("- Review response for potential bias")
                        elif "toxicity" in metric_name:
                            suggestions.append("- Ensure response is appropriate and non-toxic")
                        elif "completeness" in metric_name:
                            suggestions.append("- Provide more comprehensive answers")
                        elif "coherence" in metric_name:
                            suggestions.append("- Improve response structure and readability")
            
            if not suggestions:
                return "Response quality is good! No specific improvements needed."
            
            return "Suggestions for improvement:\n" + "\n".join(suggestions)
            
        except Exception as e:
            self.logger.error(f"Error generating suggestions: {str(e)}")
            return f"Error generating suggestions: {str(e)}"
    
    async def process_evaluation_request(
        self, 
        query: str, 
        response: str, 
        context: Optional[List[str]] = None,
        agent_name: str = "unknown",
        evaluation_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """
        Process an evaluation request and return results.
        
        Args:
            query: The original query
            response: The response to evaluate
            context: Context documents (optional)
            agent_name: Name of the agent that generated the response
            evaluation_type: Type of evaluation (rag, llm, comprehensive)
        
        Returns:
            Evaluation results
        """
        try:
            self.logger.info(f"Processing evaluation request for {agent_name}")
            
            if evaluation_type == "rag":
                return await self.evaluate_rag_quality(query, response, context or [], agent_name)
            elif evaluation_type == "llm":
                return await self.evaluate_llm_quality(query, response, agent_name)
            else:  # comprehensive
                return await self.evaluate_comprehensive(query, response, context, agent_name)
                
        except Exception as e:
            self.logger.error(f"Error processing evaluation request: {str(e)}")
            return {
                "error": str(e),
                "agent_name": agent_name,
                "query": query,
                "response": response
            }
